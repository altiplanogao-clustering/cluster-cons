---

- set_fact:
    spark_path: '{{ spark_conf.install_dir }}/{{ spark_res.basename}}'
# delete spark installation, configuration and data
- name: delete spark installation
  file: path="{{ spark_path }}" state=absent
  when: (fresh_unzip is defined) and fresh_unzip
- name: delete spark_res_dir
  file: path="{{ spark_res_dir }}" state=absent
  when: (fresh_unzip is defined) and fresh_unzip

# install spark files
- name: mkdir spark
  file: path="{{ spark_conf.install_dir }}" state=directory
- name: upload spark tar file
  copy:
    src: "{{ role_path }}/{{ rcpath }}/{{ spark_res.tar_file }}"
    dest: "{{ spark_conf.install_dir }}/{{ spark_res.tar_file }}"
- name: check unarchive spark directory
  stat: path="{{ spark_path }}"
  register: sparkfstat
- name: unarchive spark
  unarchive:
    src: "{{ spark_conf.install_dir }}/{{ spark_res.tar_file }}"
    dest: "{{ spark_conf.install_dir }}"
    remote_src: True
  when: sparkfstat.stat.isdir is not defined

# export HADOOP environment variables
- name: export SPARK_HOME ...
  blockinfile:
    path: /etc/environment
    marker: "# {mark} SPARK BLOCK"
    block: |
      SPARK_HOME={{ spark_path }}
- name: export SPARK_HOME ... 
  blockinfile:
    path: "{{ sys_profile }}"
    marker: "# {mark} SPARK BLOCK"
    block: |
      export SPARK_HOME={{ spark_path }}

#  when: ansible_os_family == "RedHat"
- name: export SPARK_HOME to PATH
  lineinfile:
    dest: '{{ sys_profile }}'
    line: 'export PATH=$PATH:$SPARK_HOME/bin'
# - name: fetch JAVA_HOME
#   command: /bin/bash -l -c "echo $JAVA_HOME"
#   changed_when: false
#   register: fetch_java_home
# - debug: msg="JAVA_HOME={{ fetch_java_home.stdout}}"
# - name: set spark JAVA_HOME
#   lineinfile:
#     dest: "{{ spark_path }}/etc/spark/spark-env.sh"
#     regexp: '^export JAVA_HOME='
#     line: 'export JAVA_HOME={{ fetch_java_home.stdout}}'

